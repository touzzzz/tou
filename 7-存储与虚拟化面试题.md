**存储与虚拟化面试题**

---------------基础类-------------

**一块裸磁盘需要哪些流程才能写入数据？**
答案：分区、格式化、挂载

**分区的作用？**

答案：

组织与管理数据：分区允许用户将硬盘空间划分为多个独立的逻辑区域，每个区域可以用来存放不同类型的文件和数据。

提升系统性能：通过合理安排分区，可以减少数据访问时磁头移动的距离，从而加快读写速度。

增强数据安全性：分区能够隔离数据，当操作系统分区出现问题（如病毒感染、系统崩溃）时，不会直接影响到存储在其他分区（如数据分区）的文件，降低了数据丢失的风险。同时，对重要数据进行单独分区并定期备份，也是数据保护策略的一部分。

支持多操作系统：在单个硬盘上安装多个操作系统变得可能，每个操作系统安装在不同的分区上。这使得用户能够在同一台计算机上选择启动进入不同的操作系统环境，满足多样化的需求。

适应技术限制与兼容性：在早期，由于文件系统和BIOS的限制，分区是必需的，比如FAT文件系统对磁盘大小的限制以及旧式BIOS对启动分区位置的约束。虽然现代技术已逐渐克服这些限制，分区仍然作为兼容性和灵活性的手段保留下来。

便于磁盘维护与扩展：分区使磁盘维护任务（如碎片整理、磁盘检查）更加高效，特别是对于较小的分区。同时，通过扩展或重新分配分区空间，可以灵活应对存储需求的变化，尽管这在GPT和LVM（逻辑卷管理）等现代技术中变得更加灵活。

**什么是磁盘的分区表，有哪些类型**

答案：磁盘的分区表是位于磁盘上的一个特殊数据结构，它负责记录磁盘上各个分区的信息，包括分区的起始位置、结束位置、类型以及一些其他属性。分区表允许一个物理磁盘被逻辑上划分为多个独立的区域，每个区域都可以被操作系统当作单独的卷或驱动器来管理和使用，从而实现数据的组织和管理。

磁盘分区表主要有两种类型：

MBR（Master Boot Record，主引导记录）分区表：

MBR分区表是较早的分区方式，适用于大多数传统的BIOS系统和一些较旧的操作系统。

在MBR分区表中，分区信息存储在磁盘的第一个扇区（MBR扇区）中，该扇区还包括引导加载程序和分区表本身。MBR分区表仅能定义4个主分区，或者3个主分区加上一个扩展分区（在扩展分区中可以创建多个逻辑分区）。

MBR分区表的最大支持容量为2TB，且不支持现代的UEFI启动。

GPT（GUID Partition Table，全局唯一标识符分区表）：

GPT是随着UEFI规范引入的一种更先进的分区表格式，适合于大容量硬盘和现代操作系统。

GPT分区表不再局限于MBR的4个分区限制，理论上可以支持几乎无限数量的分区（实际受操作系统限制）。

GPT使用一个更为复杂的结构，包括一个主要的GPT头、一个备份的GPT头（位于磁盘的尾部）、以及分区条目数组。每个分区都有一个唯一的GUID来标识其类型。

GPT分区表支持的磁盘容量远超MBR，可达EB（Exabyte）级，并且提供更好的数据完整性校验，以及支持UEFI启动。

总的来说，MBR分区表适用于较旧的系统或对兼容性有特定需求的场景，而GPT分区表则因其扩展性和高级功能成为现代系统和大容量硬盘的首选

**问题: 如何查看Linux系统的磁盘使用情况？**

解答: 使用df -h命令可以查看各文件系统的磁盘使用情况，包括总容量、已用空间、可用空间以及挂载点，-h参数表示以人类可读的格式（如KB, MB, GB）显示。

**问题: 若一台计算机的内存为8GB，交换分区的大小通常是多少？**

解答: 传统推荐是内存大小的1.5倍到2倍，但随着现代操作系统的内存管理改进，对于8GB内存的系统，通常设置交换分区大小为8GB（即等于内存大小）或稍微少一些，如4GB至8GB之间，具体取决于系统的预期用途和负载。

**问题: 如何创建一个逻辑卷？**

解答: 创建逻辑卷的基本步骤如下：

使用pvcreate命令创建物理卷（Physical Volume）。

使用vgcreate命令基于物理卷创建卷组（Volume Group）。

使用lvcreate命令在卷组上创建逻辑卷（Logical Volume）。 例如：

pvcreate /dev/sdb1

vgcreate myvg /dev/sdb1

lvcreate -L 10G -n mylv myvg

**问题: 如何挂载一个名为mydata的分区到/mnt/data目录下？**

解答: 使用mount命令挂载，假设分区设备名为/dev/sdc1：

mount /dev/sdc1 /mnt/data

若要使其在系统启动时自动挂载，需要编辑/etc/fstab文件。

**问题: 如何设置用户在某一文件系统上的磁盘配额？**

解答: 首先，确保文件系统已经启用了配额支持，然后使用quotacheck生成配额文件，接着用edquota编辑具体的配额限制，例如限制用户user1在/home目录下的磁盘使用：

quotacheck -cvug /home

edquota -u user1

**问题: 如何检测并修复文件系统错误？**

解答: 可以使用fsck命令检测并修复文件系统错误，例如检查根分区（需要在单用户模式或使用live CD/USB）：

fsck -y /dev/sda1


**简述raid0、raid1、raid5三种工作模式的工作原理及特点？**

RAID 0 (Striping)

工作原理：

RAID 0通过条带化（striping）技术将数据分割成相同大小的数据块（chunks），并将这些数据块均匀地分布到组成阵列的所有硬盘上。当写入数据时，数据块按顺序依次写入到各个硬盘上，形成连续的数据流。读取数据时，阵列能够并行从各个硬盘上读取对应的数据块，然后重组为完整的数据。

特点：

高性能：由于数据读写操作可以在多个硬盘上同时进行，RAID 0提供了最高的存储性能，特别适合对读写速度要求高的应用场景。

无冗余：RAID 0不提供数据冗余，因此不具备错误恢复能力。任何一个硬盘发生故障，都会导致整个阵列中的所有数据丢失。

容量利用率：RAID 0的总存储容量等于所有成员硬盘容量之和，没有用于冗余的数据开销，因此容量利用率最高。

RAID 1 (Mirroring)

工作原理：

RAID 1采用镜像（mirroring）方式，将数据完全复制到阵列中的两块或多块硬盘上。任何时候，同一份数据都存在于至少两个硬盘上，保持完全一致。

特点：

数据冗余：RAID 1提供最高的数据保护级别，任意一块硬盘出现故障时，系统可以从镜像硬盘中立即读取数据，保证数据的完整性和可用性。

读性能：由于数据有副本，读取操作可以同时从两块硬盘中进行，理论上可以提高读取性能，尤其是在随机读取密集型应用中。

容量利用率：RAID 1的总存储容量等于最小硬盘容量的两倍（对于多硬盘镜像则相应增加），因为一半的硬盘空间用于存储冗余数据，故容量利用率较低。

写性能：写入操作需要同时写入两块硬盘，虽然存在同步开销，但对于大多数应用而言，其写性能通常仍能满足需求。

RAID 5 (Block-Level Striping with Distributed Parity)

工作原理：

RAID 5结合了条带化与奇偶校验（parity）技术。数据同样被分割成块并分布在多个硬盘上，但除了存放数据块外，还有一块硬盘用于存储奇偶校验信息。奇偶校验数据跨越所有硬盘，分散存储，使得任何单一硬盘故障时，可以根据剩余硬盘上的数据块和相应的奇偶校验信息重建丢失的数据。

特点：

数据冗余：RAID 5能够容忍单个硬盘故障，通过计算丢失硬盘的数据块的奇偶校验信息，可以恢复数据，从而提供一定程度的数据冗余。

容量利用率：相较于RAID 1，RAID 5的容量利用率较高，仅需牺牲一个硬盘容量作为奇偶校验存储，而非像RAID 1那样完全镜像。总存储容量等于(n-1)块硬盘容量之和，其中n为阵列中硬盘的数量。

读性能：与RAID 0类似，RAID 5在读取操作时能实现并行访问，提供较高的读取性能。

写性能：RAID 5在写入操作时涉及奇偶校验的更新，需要额外的计算和磁盘寻址，特别是在发生硬盘故障后的重构期间，写性能可能会显著下降。不过，现代RAID控制器通过缓存、预读/预写等技术可以一定程度上缓解这一问题。

总结来说，RAID 0专注于提供最高的存储性能，但不提供数据冗余，适用于对性能要求极高且能接受数据丢失风险的场景；RAID 1以数据安全性为核心，通过完全镜像提供最高级别的数据保护，适用于对数据完整性要求极高的应用，但牺牲了存储容量；RAID 5则是折衷方案，兼顾了性能、冗余和容量利用率，适用于需要一定数据保护且注重性价比的应用场景。

**问题: 如何创建一个RAID 5阵列？**

mdadm -C /dev/md7 -n 3  -l 5 -x 1 /dev/sd[bcde]


------------------------服务类-----------------

**常见的存储方案有哪些？**

1、直接附加存储（DAS）：这是一种将存储设备直接连接到服务器的方法。DAS具有较高的性能，但扩展性较差，因为随着存储容量的增加，服务器负载也会增加。

2、网络附加存储（NAS）：NAS是一种将存储设备连接到网络中的方法。NAS设备可以像文件服务器一样在网络中共享文件。NAS具有较好的可扩展性和易于管理的特点。

3、存储区域网络（SAN）：SAN是一种将存储设备连接到专用网络中的方法。SAN具有高性能、可扩展性和灵活性，但需要较高的技术和管理水平。

**Nfs服务的作用？**

NFS（Network File System）是一种网络文件系统协议，它允许网络中的计算机之间通过网络共享文件和目录，就像它们是本地文件系统的一部分一样。

**Nfs服务为什么依赖RPC服务？**

在启动 NFS server 之前首先要启动 RPC 服务（portmap 服务）否则 NFS server 就无法向 RPC 服务去注册如果 RPC 服务重启，原来已经注册好的 NFS 端口数据会全部丢失，因此 NFS 服务也要重启以便重新向 RPC 注册。

**简述nfs工作过程？**
首先 server 启动 RPC 服务，并开启 111 端口

接着 server 启动 NFS 服务，并向 RPC 注册端口信息

client 启动 RPC（portmap服务），向 server 的 RPC (portmap) 服务发送请求，请求 server 的 NFS 端口

server 的 RPC(portmap) 服务返回 NFS 端口信息给 client

client 通过获取的 NFS 端口来建立和 server 的 NFS 连接并进行数据的传输

**Nfs服务如何配置？**

在/etc/exports文件中添加一行来共享目录：

/path/to/share 192.168.1.0/24(rw,sync,no\_subtree\_check)

**Nfs客户端如何使用？**

mount -t nfs server\_ip:/path/to/share /local/mount/point

**块存储和文件存储的区别？**

块存储：

数据访问方式：块存储提供原始的、无结构的存储空间，数据按块（通常是512字节、4KB或更大）的形式存储和访问，类似于直接操作硬盘。它不关心文件系统或文件层次结构，而是将存储空间视为连续的地址空间供上层软件（如操作系统、数据库）直接管理和组织数据。

使用者：块存储的直接使用者是操作系统、数据库管理系统或其他能够直接操作块设备的软件，而不是最终用户。这些软件系统负责在块存储基础上构建文件系统、管理数据的逻辑结构和元数据。

应用场景：适用于需要高性能、低延迟访问和精细控制存储资源的场景，如数据库、虚拟机磁盘、大型企业应用等。块存储能够提供更高的I/O吞吐量和更低的延迟，适合处理大量随机访问请求。

优点：提供了较高的性能和灵活性，更适合对I/O性能有严格要求的应用。支持快照、克隆等高级存储特性。

缺点：管理相对复杂，需要用户自己管理文件系统和数据布局。且通常不支持跨平台的文件共享。

文件存储

数据访问方式：文件存储提供了一个有结构的、层级化的目录和文件系统，用户或应用程序通过文件路径和名称来访问数据。文件存储管理文件的组织、权限和元数据。

使用者：文件存储的使用者通常是终端用户或应用程序，通过标准的文件操作API（如POSIX）进行读写操作，无需直接管理底层的块设备。

应用场景：适用于需要共享文件访问、简化数据管理的场景，如办公文档存储、多媒体文件共享、备份和归档等。文件存储更加直观易用，便于管理和维护。

优点：易于理解和使用，支持多用户共享访问，有较好的兼容性和跨平台性，通常包含丰富的访问控制和权限管理功能。

缺点：相比块存储，文件存储在处理大量小文件或高并发I/O请求时性能可能较低，且对大数据块的连续读写优化不足。

**什么是lun？**

答案：LUN是Logical Unit Number（逻辑单元号）的缩写，是存储领域中的一个核心概念。在SCSI（Small Computer System Interface）协议和存储网络（如SAN，Storage Area Network）环境中，LUN是一个用于唯一标识和访问存储系统中的逻辑单元的数字。这些逻辑单元代表了可供主机系统（如服务器）使用的存储资源，如磁盘驱动器、磁盘阵列或其上的某个分区。

每个LUN都可以被看作是一个独立的存储卷或逻辑磁盘，操作系统会将其当作直接连接的硬盘一样进行读写操作，而实际上它可能是物理存储介质上经过虚拟化和抽象化的一块区域。通过LUN，存储管理员可以灵活地分配存储空间，实施数据保护策略（如RAID），以及提供给不同的主机或应用使用，而无需关心底层物理存储的具体细节。

**什么是ipSAN?**

答案：IP SAN（Internet Protocol Storage Area Network）是一种基于IP网络的存储区域网络技术，它利用iSCSI（Internet Small Computer System Interface）协议在TCP/IP网络上传输SCSI命令，从而使数据存储和访问操作能够在标准的以太网上进行。


**什么是对象存储？**

对象存储是一种现代化的数据存储架构，它将数据作为独立的对象进行管理和存储，与传统的文件存储和块存储模型不同。在对象存储系统中，每个数据单元（即对象）不仅仅包含实际的数据内容，还包括了元数据（描述数据的信息，如创建日期、修改时间、访问权限等）和一个全局唯一的标识符（通常是UUID或自定义的命名空间中的名称）。这个标识符使得用户或应用程序可以直接通过该ID访问存储的对象，而无需了解数据的物理存储位置。

**对象存储的特点？**

扁平化结构：与文件存储中的目录树结构不同，对象存储不使用文件夹层级，所有对象都存储在一个单一的、无层次的命名空间内。

大规模可扩展性：设计上易于水平扩展，通过添加更多的存储节点可以线性增加存储容量和处理能力，非常适合处理海量数据存储需求。

高性能和高并发：优化了大量数据的读写操作，特别是在处理大量小文件和高并发请求时表现出色。

数据持久性和可靠性：通过数据冗余（如三副本或纠删码）和自动故障恢复机制确保数据的高可用性和持久性。

基于RESTful API的访问：用户通过HTTP/HTTPS协议和标准化的REST API与存储服务交互，简化了跨平台和网络的数据访问，特别适合云存储环境。

安全性：提供数据加密、访问控制列表（ACL）、身份验证等安全措施来保护数据的隐私和完整性。

**什么是分布式存储？**

答案：分布式存储是一种数据存储架构，它通过网络将数据分散在多台独立的计算机节点（服务器）上，而不是集中存储在单一设备上。这种技术的核心理念是利用集群中每个节点的本地存储资源，通过软件层将这些资源统一管理起来，形成一个逻辑上连续、功能完整的存储系统。

**简述Ceph的优势及其特点？**

Ceph是一个分布式的数据对象存储，Ceph相对其他存储系统具有如下优势：

1、CRUSH算法：ceph摒弃了传统的集中式存储元数据寻址的方案，而使用CRUSH算法完成数据的寻址操作。能够实现各类负载的副本放置规则，例如跨机房、机架感知等。Crush算法有相当强大的扩展性，理论上支持数千个存储节点，从而增强了Ceph弹性扩展和高可用性。

2、高可用：通过CRUSH算法指定副本的物理存储位置以分隔故障域，支持数据强一致性，ceph可以忍受多种故障场景并自动尝试并行修复。

3、高扩展性：Ceph本身并没有主控节点，扩展起来比较容易，并且理论上，它的性能会随着磁盘数量的增加而线性增长。

4、特性丰富：Ceph支持三种调用接口：对象存储，块存储，文件系统挂载。三种方式可以一同使用。Ceph主要特点如下：

5、统一存储；

6、无任何单点故障；

7、数据多份冗余；

8、存储容量可扩展；

9、自动容错及故障自愈。

**简述Ceph存储体系架构？**

Ceph体系架构主要由RADOS和RADOS GW和RBD以及CephFS构成。

1、RADOS（Reliable, Autonomic Distributed Object Store）是Ceph的底层核心，RADOS本身也是分布式存储系统，CEPH所有的存储功能都是基于RADOS实现。RADOS由两个组件组成：OSD和Monitor。

①OSD主要提供存储资源，每一个disk、SSD、RAID group或者一个分区都可以成为一个OSD，而每个OSD还将负责向该对象的复杂节点分发和恢复；

②Monitor维护Ceph集群并监控Ceph集群的全局状态，提供一致性的决策。

2、RADOS GW和RBD：RADOS GateWay、RBD其作用是在librados库的基础上提供抽象层次更高、更便于应用或客户端使用的上层接口。其中，RADOS GW是一个提供与Amazon S3和Swift兼容的RESTful API的gateway，以供相应的对象存储应用开发使用。RBD则提供了一个标准的块设备接口，常用于在虚拟化的场景下为虚拟机创建volume。

3. CEPHFS：CEPHFS则提供了POSIX接口，用户可直接通过客户端挂载使用。

   **简述ceph pool 有几种类型？**

   Ceph存储池Pool是Ceph存储集群用于存储对象的逻辑分区。池类型确定池用于确保数据持久性的保护机制，Ceph有两种Pool类型：

   replication类型：在集群中分布每个对象的多个副本。

   erasure coding类型：将每个对象分割成块，并将它们与其他擦除编码块一起分发，以使用自动纠错机制保护对象。

   **简述Ceph Pool、PG、OSD的关系？**

   Ceph存储池Pool是Ceph存储集群用于存储对象的逻辑分区。

   Pool中存在一定的数量的PG，PG将对象存储在一组由CRUSH算法确定的osd中。

   Ceph使用CRUSH算法将对象分配给池中的一个PG，根据池的配置和CRUSH算法，PG自动映射到一组OSDs。

   一个PG里包含一堆对象，一个对象只能属于一个PG。

   **简述Ceph节点的角色？**

   所有Ceph存储集群的部署都始于部署一个个Ceph节点、网络和Ceph存储集群。Ceph存储集群至少需要一个Ceph Monitor和两个OSD守护进程。而运行Ceph文件系统客户端时，则必须要有元数据服务器（Metadata Server）。

   1、Ceph OSDs：Ceph OSD守护进程（ Ceph OSD ）的功能是存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他OSD守护进程的心跳来向Ceph Monitors提供一些监控信息。当Ceph存储集群设定为有2个副本时，至少需要2个OSD守护进程，集群才能达到active+clean状态（Ceph默认有3个副本）。

   2、Monitors：Ceph Monitor维护着展示集群状态的各种图表，包括监视器图、OSD图、归置组（PG）图、和CRUSH 图。

   3、MDSs：Ceph元数据服务器（MDS）为Ceph文件系统存储元数据（也就是说，Ceph块设备和Ceph 对象存储不使用MDS）。元数据服务器使得POSIX文件系统的客户端，可以在不对Ceph存储集群造成负担的前提下，执行诸如ls、find等基本命令。

   **Ceph可以提供哪些类型的存储？**

   Ceph提供对象存储、块存储和文件存储。

   **Ceph的核心组件有哪些？它们各自的作用是什么？**

   Monitor：负责维护集群的映射信息和监控集群状态，保证整个系统的数据一致性。

   Manager：管理集群的日常操作，如调整OSD的PG（Placement Group）分布，以及执行用户或系统策略。

   Object Storage Device (OSD)：实际存储数据的守护进程，执行存储和检索数据的操作。

   Metadata Server (MDS)：在Ceph的文件系统（CephFS）中，负责管理文件系统的元数据。

   **解释一下Ceph的CRUSH算法。**

   CRUSH（Controlled Replication Under Scalable Hashing）算法是一种确定性的数据分布算法，用于决定数据在存储集群中的位置。它避免了集中式的元数据服务器，通过计算而非查询来定位数据，确保了数据分布的均匀性和可扩展性，同时也支持数据的复制策略。

   **Ceph如何实现高可用性？**

   Ceph通过数据复制（默认为三副本）和CRUSH算法确保数据的冗余存储。当某一部分硬件出现故障时，Ceph能自动检测并重新分布数据，确保数据的持续可用。

   **Ceph中的RADOS是什么？**

   RADOS（Reliable Autonomic Distributed Object Store）是Ceph的底层存储系统，它是一个分布式对象存储，提供了数据的可靠、自适应和分布式的存储能力，是Ceph高可靠性、高性能的基础。

   **块存储在Ceph中的应用实例有哪些？**

   在OpenStack环境中，Ceph块设备存储常用于Cinder后端，提供虚拟机的磁盘卷；也可以作为Glance的镜像存储，或者直接挂载给虚拟机使用作为其根文件系统。

   **Ceph与传统SAN/NAS存储相比有哪些优势？**

   相比于传统存储，Ceph更易于扩展、成本更低，因为它基于通用硬件；提供了更高的灵活性和动态可配置性；支持多协议访问（对象、块、文件）；且具有更好的容错和恢复机制。

   **如何在Ceph集群中添加一个新的OSD节点？**

   步骤大致包括：准备硬件和安装Ceph软件；使用ceph-deploy或Cephadm工具加入新节点；创建和准备 OSD 存储设备；最后使用命令将新OSD加入集群并激活

   ----------------------虚拟化模块---------

   **什么是虚拟化技术**

   虚拟化技术是一种资源管理技术，是将计算机的各种实体资源（CPU、内存、磁盘空间、网络适配器等），予以抽象、转换后呈现出来并可供分割、组合为一个或多个电脑配置环境。

   **虚拟化的作用？**

   资源优化与提高利用率

   成本节约

   灵活性与敏捷性无需中断服务

   简化管理

   提高可用性和灾备能力

   安全隔离

   **你接触过哪些虚拟化产品？**

   VMware vSphere/ESXi：VMware vSphere 是一个企业级的虚拟化平台，以其稳定性、性能和全面的管理工具著称。ESXi 是其裸金属型的hypervisor，直接安装在硬件上，无需操作系统支持。

   Microsoft Hyper-V：Hyper-V 是微软提供的服务器虚拟化解决方案，与Windows Server紧密集成，特别适合Windows环境，支持动态迁移等功能。

   Citrix XenServer：基于开源Xen项目的虚拟化平台，适合需要高级资源管理和迁移功能的环境。

   Red Hat Virtualization (RHV)：基于KVM技术的企业级虚拟化解决方案，与Red Hat生态系统集成紧密，提供出色的Linux支持。

   Oracle VM：Oracle提供的服务器虚拟化平台，基于Xen，特别适合与Oracle数据库和其他Oracle产品集成的环境。

   KVM：一个开源的系统虚拟化模块，使用Linux自身的调度器进行管理。

   **简单介绍KVM？**

   KVM是Kernel-based Virtual Machine的简称，一个开源的系统虚拟化模块，使用Linux自身的调度器进行管理，KVM的虚拟化需要硬件支持（如Intel VT技术或者AMD V技术)。是基于硬件的完全虚拟化。

   **kvm的三个组件及作用**

   kvm：负责cpu虚拟化+内存虚拟化

   qemu：负责IO的虚拟化

   libvirt：是调用kvm虚拟化技术的接口用于管理的

   **磁盘镜像格式raw和qcow2的区别**

   raw：创建时占用全部容量，不支持动态扩容，不支持快照，性能好

   qcow2：写时复制，开始只占用少许容量，支持动态扩容，性能不如raw

   **虚拟机文件中配置文件和硬盘文件默认分别在哪个路径下**

   虚拟机配置文件，XML文件，位置 ：/etc/libvirt/qemu/

   虚拟机硬盘文件，位置：/var/lib/libvirt/images/

   **kvm虚拟机的网络配置有哪两种模式？默认使用哪一种？**

   NAT模式：也是用户模式，数据包由NAT方式通过主机的接口进行传送，可以访问公网，但是无法从外部访问虚拟机网络，kvm默认用的这种网络。

   Bridge：也就是桥接模式，这种模式允许虚拟机像一个独立的主机一样拥有网络，外部的机器可以直接访问到虚拟机内部，但需要网卡支持，一般有线网卡都支持。

   **kvm支持哪些虚拟磁盘格式?**

   kvm从qemu继承了丰富的磁盘格式, 包括裸映象(raw images), 原始qemu格式(qcow), VMware格式和更多

   **kvm和VMware有何区别?**

   VMware是一个专利产品.，企业级的是收费的。

   kvm是一个开源的系统虚拟化软件。

   **------KVM虚拟化常用命令---------**

   vm为虚机的名字

   #列出正在运行的虚机

   virsh list

   #列出所有的虚拟机

   virsh list --all

   #查看virsh的版本

   virsh version

   #启动虚机

   virsh start vm

   #关闭虚机

   virsh shutdown vm

   #强制性关闭该虚拟机，相当于强行断电

   virsh destroy vm

   #将虚机的配置文件导出到/tmp/目录下，并命名为vm.xml

   virsh dumpxml centos7-2 > /tmp/vm.xml

   #虚拟机自启动

   virsh autostart vm

   #虚拟机自启动撤销

   virsh autostart --disable vm

   #虚拟机克隆

   virt-clone -o 【原虚拟机】 -n 【新虚拟机】 -f 【新虚拟机镜像名（含路径）】

   virt-clone -o vm-1 -n vm-2 -f /var/lib/libvirt/images/vm-2.img

   #查看虚拟机镜像

   qemu-img info vm.img

   #创建快照

   virsh snapshot-create vm

   #列出虚机快照

   virsh snapshot-list vm

   qemu-img info vm

   #指定虚拟机名字创建快照

   virsh snapshot-create-as 【虚拟机名】 【快照名】

   virsh snapshot-create-as vm snapshot-vm

   #查看该虚拟机当前使用的快照

   virsh snapshot-current vm

   #恢复快照

   virsh snapshot-revert 【虚拟机名】 【快照名】

   virsh snapshot-revert vm snapshot-vm

   #删除快照

   virsh snapshot-delet vm snapshot-vm

   #定义存储池与其目录

   virsh pool-define-as vmdisk --type dir --target /data/vmfs

   #创建已定义的存储池

   virsh pool-build vmdisk

   #激活已定义的存储池

   virsh pool-start vmdisk

   #自动启动已定义的存储池

   virsh pool-autostart vmdisk

   #查看已定义的存储池

   virsh pool-list --all

   #在存储池中创建虚拟机存储卷

   virsh vol-create-as vmdisk test.qcow2 3G --format qcow2

